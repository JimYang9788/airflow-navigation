{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Introduction \n",
    "\n",
    "#### 1. Design Conepts\n",
    "Airflow is a platform to programmatically author, schedule and monitor workflows.  \n",
    "\n",
    "Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap.\n",
    "\n",
    "Airflow also has a rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. DAG\n",
    "Airflow relies on the concept of DAG. In mathematics and computer science, a directed acyclic graph, is a finite directed graph with no directed cycles. \n",
    "\n",
    "DAG is **one directional**, that is, it consists of finitely many vertices and edges, with each edge directed from one vertex to another, such that there is no way to start at any vertex v and follow a consistently-directed sequence of edges that eventually loops back to v again. Equivalently, a DAG is a directed graph that has a topological ordering, a sequence of the vertices such that every edge is directed from earlier to later in the sequence.\n",
    "\n",
    "A DAG has many desired properties working a a graph problems. For instance, a DAG can be topologically sorted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Operator \n",
    "\n",
    "In Airflow all workflows are DAGs. A Dag consists of operators. An operator defines an individual task that needs to be performed. There are different types of operators available. Operator determines what actually gets done. Once an operator is instantiated, it is referred to as a \"task\". An operator describes a single task in a workflow. \n",
    "\n",
    "Categories of operator\n",
    " \n",
    "1. **Sensor**: A type of operator that will keep running until a certain criteria is met. Example include waiting for a certain time, external file, or upstream data source.   \n",
    "    e.g. Hdfs Sensor: wait for a file or folder to land in HDFS  \n",
    "    e.g. NamedHivePartitionSensor: Check whether the most recent partition of a Hive table \n",
    "2. **Operator**: triggers a certain action (e.g. run a bash command, execute a python function, or execute a Hive query, etc.   \n",
    "    e.g. PythonOperator   \n",
    "    e.g. BashOperator  \n",
    "3. **Transfer**: Move data from one location to another  \n",
    "    e.g. MySQLtoHiveTransfer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Example\n",
    "Install airflow using:\n",
    "```\n",
    "pip install apache-airflow\n",
    "\n",
    "```\n",
    "First, import all the needed airflow files\n",
    "```\n",
    "from datetime import timedelta\n",
    "\n",
    "import airflow \n",
    "from airflow import DAG \n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "import os \n",
    "\n",
    "```\n",
    "Airflow allows one to supply default arguments. Here in the example, we included ```owner```, ```start_date```, ```concurrency``` and ```retries```. \n",
    "\n",
    "```\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': dt.datetime(2018, 9, 24, 10, 00, 00),\n",
    "    'concurrency': 1,\n",
    "    'retries': 0\n",
    "}\n",
    "```\n",
    "\n",
    "```start_date``` tells you when the workflow starts, \n",
    "```concurrency``` tells one to dictate the number of processes needs to be used running multiple DAGs. \n",
    "```retries``` tells users how many time the process will retry after it fails. A more detailed documutation for default arguments can be found here https://airflow.apache.org/docs/stable/concepts.html\n",
    "\n",
    "Then, we can create a DAG using this simple default argument set:\n",
    "```\n",
    "with DAG('my_simple_dag',\n",
    "         default_args=default_args,\n",
    "         schedule_interval='*/10 * * * *',\n",
    "         ) as dag:\n",
    "    opr_hello = BashOperator(task_id='say_Hi',\n",
    "                             bash_command='echo \"Hi!!\"')\n",
    "\n",
    "    opr_greet = PythonOperator(task_id='greet',\n",
    "                               python_callable=greet)\n",
    "    opr_sleep = BashOperator(task_id='sleep_me',\n",
    "                             bash_command='sleep 5')\n",
    "\n",
    "    opr_respond = PythonOperator(task_id='respond',\n",
    "                                 python_callable=respond)\n",
    "opr_hello >> opr_greet >> opr_sleep >> opr_respond\n",
    "```\n",
    "\n",
    "Here in the example we have four operators, namely:\n",
    "\n",
    "```opr_hello``` A bash operator  \n",
    "```opr_greet``` A python operator   \n",
    "```opr_sleep``` A bash operator  \n",
    "```opr_respond``` A a python operator  \n",
    "\n",
    "Each operator corresponds to a task in the workflow. Once they are initiated as an instance, they become part of the workflow. \n",
    "\n",
    "They are then connected by bitwise operator ```>>``` to indicate the order of execution. Here the order is dictated by \n",
    "```opr_hello >> opr_greet >> opr_sleep >> opr_respond```, indicating the order is \n",
    "opr_hello -> opr_greet -> opr_sleep -> opr_respond\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
